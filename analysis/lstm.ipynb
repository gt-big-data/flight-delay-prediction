{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('2018.csv')\n",
    "top_10_groups = df.groupby(['ORIGIN', 'DEST', 'OP_CARRIER']).size().sort_values(ascending=False).head(50)\n",
    "print(top_10_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "SELECTED_ORIGIN = 'LGA'\n",
    "SELECTED_DEST = 'ATL'\n",
    "SELECTED_OP_CARRIER = 'DL'\n",
    "start_year = 2018\n",
    "end_year = 2018\n",
    "\n",
    "def get_df(origin, dest, carrier, start_year, end_year, exclude=None):\n",
    "    ans = None\n",
    "    for year in range(start_year, end_year+1):\n",
    "        df = pd.read_csv(f\"{year}.csv\")\n",
    "        if ans is None:\n",
    "            ans = df[(df['OP_CARRIER'] == carrier) & (df['ORIGIN'] == origin) & (df['DEST'] == dest)]\n",
    "            continue\n",
    "        ans = pd.concat([ans, df[(df['OP_CARRIER'] == carrier) & (df['ORIGIN'] == origin) & (df['DEST'] == dest)]])\n",
    "    \n",
    "    return ans\n",
    "\n",
    "\n",
    "df = get_df(SELECTED_ORIGIN, SELECTED_DEST, SELECTED_OP_CARRIER, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.reset_index()\n",
    "\n",
    "filtered_df['DEP_DELAY'] = filtered_df['DEP_DELAY'].apply(lambda x: max(0,x))\n",
    "\n",
    "# Convert the time column to a string and format it\n",
    "filtered_df['CRS_DEP_TIME'] = filtered_df['CRS_DEP_TIME'].apply(lambda x: '{:04.0f}'.format(x))\n",
    "filtered_df['CRS_DEP_TIME'] = filtered_df['CRS_DEP_TIME'].str[:2] + ':' + filtered_df['CRS_DEP_TIME'].str[2:]\n",
    "\n",
    "# Combine date and time columns\n",
    "filtered_df['datetime'] = pd.to_datetime(filtered_df['FL_DATE'] + ' ' + filtered_df['CRS_DEP_TIME'])\n",
    "DROPPED_COLUMNS = ['CANCELLATION_CODE', 'CARRIER_DELAY', 'WEATHER_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'Unnamed: 27', 'NAS_DELAY', 'CANCELLED', 'DIVERTED']\n",
    "filtered_df = filtered_df.drop(columns=DROPPED_COLUMNS)\n",
    "filtered_df.dropna(inplace=True)\n",
    "\n",
    "data = filtered_df.copy()\n",
    "# Convert the time column to a string and format it\n",
    "data['CRS_DEP_TIME'] = data['CRS_DEP_TIME'].apply(lambda x: x.replace(\":\",\"\")).astype(int).apply(lambda x: '{:04.0f}'.format(x))\n",
    "data['CRS_DEP_TIME'] = data['CRS_DEP_TIME'].str[:2] + ':' + data['CRS_DEP_TIME'].str[2:]\n",
    "\n",
    "# Combine date and time columns\n",
    "data['datetime'] = pd.to_datetime(data['FL_DATE'] + ' ' + data['CRS_DEP_TIME'])\n",
    "data = data.sort_values(by='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "lat = 28.43\n",
    "long = -81.31\n",
    "response = requests.get(f'https://archive-api.open-meteo.com/v1/archive?latitude={lat}&longitude={long}&start_date={start_year}-01-01&end_date={end_year}-12-31&hourly=temperature_2m,rain,snowfall,cloudcover,windspeed_100m')\n",
    "weather = response.json()\n",
    "\n",
    "weather_dict = {}\n",
    "from datetime import datetime\n",
    "for i, date in enumerate(weather['hourly']['time']):\n",
    "    hour  = datetime.strptime(date, \"%Y-%m-%dT%H:%M\")\n",
    "    weather_dict[hour] = {'temperature_2m': weather['hourly']['temperature_2m'][i],'rain': weather['hourly']['rain'][i], 'snowfall': weather['hourly']['snowfall'][i], 'cloudcover': weather['hourly']['cloudcover'][i], 'windspeed_100m': weather['hourly']['windspeed_100m'][i] }\n",
    "\n",
    "# Floor the datetime to the nearest hour\n",
    "data[\"floored_datetime\"] = data[\"datetime\"].dt.floor(\"H\")\n",
    "\n",
    "# Look up the weather data and add new columns to the DataFrame\n",
    "for feature in ['temperature_2m', 'rain', 'snowfall', 'cloudcover', 'windspeed_100m']:\n",
    "    data[feature] = data[\"floored_datetime\"].map(lambda x: weather_dict.get(x, {}).get(feature, None))\n",
    "\n",
    "# Drop the 'floored_datetime' column if not needed\n",
    "data.drop(columns=[\"floored_datetime\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5400.000000\n",
       "mean       12.977407\n",
       "std        47.526788\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         3.000000\n",
       "max      1038.000000\n",
       "Name: DEP_DELAY, dtype: float64"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DEP_DELAY'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>...</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>snowfall</th>\n",
       "      <th>cloudcover</th>\n",
       "      <th>windspeed_100m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16914</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1447</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>06:00</td>\n",
       "      <td>606.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>160.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>13.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16479</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>904</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>07:00</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17248</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1842</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>07:59</td>\n",
       "      <td>757.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-01-01 07:59:00</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16994</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1539</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>10:00</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>170.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-01-01 10:00:00</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16284</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>658</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>13:00</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>164.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-01-01 13:00:00</td>\n",
       "      <td>14.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>15.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>7199300</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>821</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>13:00</td>\n",
       "      <td>1255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-12-31 13:00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>7196561</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>1877</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>14:00</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-12-31 14:00:00</td>\n",
       "      <td>21.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>18.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>7199187</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>645</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>15:00</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>152.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-12-31 15:00:00</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>7199728</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>1346</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>17:00</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>168.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-12-31 17:00:00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>7196527</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>1832</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>18:00</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-12-31 18:00:00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5400 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  \\\n",
       "3       16914  2018-01-01         DL               1447    LGA  ATL   \n",
       "1       16479  2018-01-01         DL                904    LGA  ATL   \n",
       "6       17248  2018-01-01         DL               1842    LGA  ATL   \n",
       "4       16994  2018-01-01         DL               1539    LGA  ATL   \n",
       "0       16284  2018-01-01         DL                658    LGA  ATL   \n",
       "...       ...         ...        ...                ...    ...  ...   \n",
       "5466  7199300  2018-12-31         DL                821    LGA  ATL   \n",
       "5463  7196561  2018-12-31         DL               1877    LGA  ATL   \n",
       "5465  7199187  2018-12-31         DL                645    LGA  ATL   \n",
       "5468  7199728  2018-12-31         DL               1346    LGA  ATL   \n",
       "5461  7196527  2018-12-31         DL               1832    LGA  ATL   \n",
       "\n",
       "     CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  ...  CRS_ELAPSED_TIME  \\\n",
       "3           06:00     606.0        6.0      16.0  ...             160.0   \n",
       "1           07:00     700.0        0.0      16.0  ...             162.0   \n",
       "6           07:59     757.0        0.0      15.0  ...             155.0   \n",
       "4           10:00    1012.0       12.0      24.0  ...             170.0   \n",
       "0           13:00    1542.0      162.0      18.0  ...             164.0   \n",
       "...           ...       ...        ...       ...  ...               ...   \n",
       "5466        13:00    1255.0        0.0      12.0  ...             161.0   \n",
       "5463        14:00    1356.0        0.0      14.0  ...             161.0   \n",
       "5465        15:00    1454.0        0.0      14.0  ...             152.0   \n",
       "5468        17:00    1658.0        0.0      12.0  ...             168.0   \n",
       "5461        18:00    1758.0        0.0      14.0  ...             153.0   \n",
       "\n",
       "      ACTUAL_ELAPSED_TIME  AIR_TIME  DISTANCE            datetime  \\\n",
       "3                   148.0     125.0     762.0 2018-01-01 06:00:00   \n",
       "1                   157.0     125.0     762.0 2018-01-01 07:00:00   \n",
       "6                   144.0     123.0     762.0 2018-01-01 07:59:00   \n",
       "4                   152.0     122.0     762.0 2018-01-01 10:00:00   \n",
       "0                   150.0     125.0     762.0 2018-01-01 13:00:00   \n",
       "...                   ...       ...       ...                 ...   \n",
       "5466                134.0     117.0     762.0 2018-12-31 13:00:00   \n",
       "5463                139.0     119.0     762.0 2018-12-31 14:00:00   \n",
       "5465                140.0     118.0     762.0 2018-12-31 15:00:00   \n",
       "5468                137.0     119.0     762.0 2018-12-31 17:00:00   \n",
       "5461                141.0     120.0     762.0 2018-12-31 18:00:00   \n",
       "\n",
       "      temperature_2m  rain  snowfall  cloudcover  windspeed_100m  \n",
       "3               13.7   0.0       0.0          64            14.6  \n",
       "1               13.9   0.0       0.0          74            12.0  \n",
       "6               13.9   0.0       0.0          74            12.0  \n",
       "4               14.6   0.2       0.0         100             6.5  \n",
       "0               14.3   1.7       0.0         100            15.7  \n",
       "...              ...   ...       ...         ...             ...  \n",
       "5466            20.0   0.0       0.0          55            19.1  \n",
       "5463            21.4   0.0       0.0          52            18.4  \n",
       "5465            22.9   0.0       0.0          65            16.8  \n",
       "5468            25.0   0.0       0.0          42            23.6  \n",
       "5461            25.8   0.0       0.0          24            22.8  \n",
       "\n",
       "[5400 rows x 26 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "135/135 [==============================] - 1s 5ms/step - loss: 2437.7551 - val_loss: 1717.2875\n",
      "Epoch 2/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2366.1658 - val_loss: 1681.8304\n",
      "Epoch 3/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2323.0610 - val_loss: 1642.7642\n",
      "Epoch 4/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2278.9424 - val_loss: 1621.7362\n",
      "Epoch 5/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2245.2236 - val_loss: 1602.3839\n",
      "Epoch 6/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2217.1672 - val_loss: 1575.1715\n",
      "Epoch 7/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2182.6365 - val_loss: 1551.3993\n",
      "Epoch 8/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2149.7693 - val_loss: 1551.2511\n",
      "Epoch 9/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2107.7744 - val_loss: 1509.1471\n",
      "Epoch 10/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2064.3938 - val_loss: 1481.4097\n",
      "Epoch 11/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2025.0508 - val_loss: 1449.8348\n",
      "Epoch 12/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1986.8402 - val_loss: 1434.1782\n",
      "Epoch 13/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1943.3604 - val_loss: 1412.4835\n",
      "Epoch 14/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 1910.3132 - val_loss: 1365.3618\n",
      "Epoch 15/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1864.3356 - val_loss: 1385.7699\n",
      "Epoch 16/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1835.0786 - val_loss: 1375.8759\n",
      "Epoch 17/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1811.6840 - val_loss: 1349.1794\n",
      "Epoch 18/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1771.6465 - val_loss: 1349.0294\n",
      "Epoch 19/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1745.6162 - val_loss: 1314.9958\n",
      "Epoch 20/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1725.6069 - val_loss: 1289.3981\n",
      "Epoch 21/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1705.9578 - val_loss: 1260.7701\n",
      "Epoch 22/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1653.7885 - val_loss: 1310.4381\n",
      "Epoch 23/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1632.8481 - val_loss: 1269.6836\n",
      "Epoch 24/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1615.3986 - val_loss: 1303.1146\n",
      "Epoch 25/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1606.8746 - val_loss: 1252.2412\n",
      "Epoch 26/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1586.7017 - val_loss: 1263.9069\n",
      "Epoch 27/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1549.2385 - val_loss: 1233.2162\n",
      "Epoch 28/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1558.4132 - val_loss: 1187.0983\n",
      "Epoch 29/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1522.0704 - val_loss: 1157.0853\n",
      "Epoch 30/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1468.6876 - val_loss: 1147.2496\n",
      "Epoch 31/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1458.0831 - val_loss: 1135.8944\n",
      "Epoch 32/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1430.1569 - val_loss: 1131.4340\n",
      "Epoch 33/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1419.9087 - val_loss: 1123.0817\n",
      "Epoch 34/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1404.6794 - val_loss: 1121.9824\n",
      "Epoch 35/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1383.0503 - val_loss: 1112.7330\n",
      "Epoch 36/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1366.4027 - val_loss: 1128.2665\n",
      "Epoch 37/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1359.2799 - val_loss: 1135.9834\n",
      "Epoch 38/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1340.6089 - val_loss: 1141.7892\n",
      "Epoch 39/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1349.5850 - val_loss: 1160.3323\n",
      "Epoch 40/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1304.7642 - val_loss: 1096.4275\n",
      "Epoch 41/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1306.5945 - val_loss: 1044.2871\n",
      "Epoch 42/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1261.1692 - val_loss: 1022.8110\n",
      "Epoch 43/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1227.9807 - val_loss: 1045.3104\n",
      "Epoch 44/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1232.2108 - val_loss: 1058.0037\n",
      "Epoch 45/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1194.3083 - val_loss: 1096.4814\n",
      "Epoch 46/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1241.1964 - val_loss: 1102.5693\n",
      "Epoch 47/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1186.0378 - val_loss: 1120.8655\n",
      "Epoch 48/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1133.4766 - val_loss: 1119.4362\n",
      "Epoch 49/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1170.4117 - val_loss: 1092.4598\n",
      "Epoch 50/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1111.8646 - val_loss: 1128.1422\n",
      "Epoch 51/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1122.2533 - val_loss: 1108.0536\n",
      "Epoch 52/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1100.0751 - val_loss: 1120.7090\n",
      "Epoch 53/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1104.0353 - val_loss: 1206.6580\n",
      "Epoch 54/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1103.0430 - val_loss: 1205.2933\n",
      "Epoch 55/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1085.6002 - val_loss: 1194.6240\n",
      "Epoch 56/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1075.1417 - val_loss: 1246.2045\n",
      "Epoch 57/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1047.6761 - val_loss: 1289.3180\n",
      "Epoch 58/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 1047.8386 - val_loss: 1203.2539\n",
      "Epoch 59/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1010.7368 - val_loss: 1270.0569\n",
      "Epoch 60/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1024.9620 - val_loss: 1226.5164\n",
      "Epoch 61/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1099.2754 - val_loss: 1236.7515\n",
      "Epoch 62/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1022.6374 - val_loss: 1252.3447\n",
      "Epoch 63/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 975.7160 - val_loss: 1250.4297\n",
      "Epoch 64/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1001.4589 - val_loss: 1272.9805\n",
      "Epoch 65/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 983.0013 - val_loss: 1189.6578\n",
      "Epoch 66/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 969.2122 - val_loss: 1210.5186\n",
      "Epoch 67/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 932.7024 - val_loss: 1223.1989\n",
      "Epoch 68/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 934.0617 - val_loss: 1224.4169\n",
      "Epoch 69/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 928.1237 - val_loss: 1201.4370\n",
      "Epoch 70/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 950.2287 - val_loss: 1185.7805\n",
      "Epoch 71/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 971.9243 - val_loss: 1102.5708\n",
      "Epoch 72/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 982.8401 - val_loss: 1260.5378\n",
      "Epoch 73/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 987.2368 - val_loss: 1250.6914\n",
      "Epoch 74/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 900.5039 - val_loss: 1152.5144\n",
      "Epoch 75/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 886.1224 - val_loss: 1192.1147\n",
      "Epoch 76/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 921.1948 - val_loss: 1093.0421\n",
      "Epoch 77/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 910.4991 - val_loss: 1268.5363\n",
      "Epoch 78/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 884.2282 - val_loss: 1145.6239\n",
      "Epoch 79/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 865.3958 - val_loss: 1255.9991\n",
      "Epoch 80/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 841.7179 - val_loss: 1211.2407\n",
      "Epoch 81/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 829.4322 - val_loss: 1185.4990\n",
      "Epoch 82/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 826.1543 - val_loss: 1190.6036\n",
      "Epoch 83/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 816.5688 - val_loss: 1177.9875\n",
      "Epoch 84/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 873.8978 - val_loss: 1235.0441\n",
      "Epoch 85/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 840.8683 - val_loss: 1270.4601\n",
      "Epoch 86/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 845.7344 - val_loss: 1235.1692\n",
      "Epoch 87/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 776.3989 - val_loss: 1134.2604\n",
      "Epoch 88/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 814.4459 - val_loss: 1261.5078\n",
      "Epoch 89/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 779.1622 - val_loss: 1104.1539\n",
      "Epoch 90/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 748.8630 - val_loss: 1174.0967\n",
      "Epoch 91/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 750.4548 - val_loss: 1169.1980\n",
      "Epoch 92/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 759.5709 - val_loss: 1162.2188\n",
      "Epoch 93/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 708.9999 - val_loss: 1086.5880\n",
      "Epoch 94/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 736.4859 - val_loss: 1174.1526\n",
      "Epoch 95/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 739.6592 - val_loss: 1057.7073\n",
      "Epoch 96/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 708.4562 - val_loss: 1169.7262\n",
      "Epoch 97/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 689.5186 - val_loss: 1157.9348\n",
      "Epoch 98/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 681.6138 - val_loss: 1211.5195\n",
      "Epoch 99/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 696.9036 - val_loss: 1184.5719\n",
      "Epoch 100/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 670.0256 - val_loss: 1167.4496\n",
      "Epoch 101/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 735.2083 - val_loss: 1187.1663\n",
      "Epoch 102/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 847.5126 - val_loss: 1686.8521\n",
      "Epoch 103/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 791.9157 - val_loss: 1161.3351\n",
      "Epoch 104/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 757.8025 - val_loss: 1226.6329\n",
      "Epoch 105/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 749.3895 - val_loss: 1253.1865\n",
      "Epoch 106/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 726.0923 - val_loss: 1074.7777\n",
      "Epoch 107/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 676.7949 - val_loss: 1076.8755\n",
      "Epoch 108/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 656.3356 - val_loss: 1023.2442\n",
      "Epoch 109/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 677.8394 - val_loss: 1166.5042\n",
      "Epoch 110/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 656.1054 - val_loss: 1128.0363\n",
      "Epoch 111/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 634.9482 - val_loss: 1082.1979\n",
      "Epoch 112/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 620.1024 - val_loss: 1109.6691\n",
      "Epoch 113/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 615.7867 - val_loss: 1098.4518\n",
      "Epoch 114/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 633.2766 - val_loss: 1090.9729\n",
      "Epoch 115/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 606.4105 - val_loss: 1104.2905\n",
      "Epoch 116/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 620.0067 - val_loss: 1239.1758\n",
      "Epoch 117/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 618.7630 - val_loss: 1022.9835\n",
      "Epoch 118/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 646.4866 - val_loss: 1066.9933\n",
      "Epoch 119/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 593.9971 - val_loss: 1100.0653\n",
      "Epoch 120/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 621.5833 - val_loss: 1077.8118\n",
      "Epoch 121/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 582.5265 - val_loss: 1035.2031\n",
      "Epoch 122/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 581.8503 - val_loss: 1095.4092\n",
      "Epoch 123/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 593.2365 - val_loss: 1123.4377\n",
      "Epoch 124/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 618.5236 - val_loss: 1033.4365\n",
      "Epoch 125/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 645.0661 - val_loss: 1105.4762\n",
      "Epoch 126/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 699.5782 - val_loss: 1244.1737\n",
      "Epoch 127/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 595.7159 - val_loss: 1043.4009\n",
      "Epoch 128/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 569.1486 - val_loss: 1049.4979\n",
      "Epoch 129/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 586.3627 - val_loss: 1064.8610\n",
      "Epoch 130/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 584.3600 - val_loss: 1056.9100\n",
      "Epoch 131/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 551.4349 - val_loss: 1090.9733\n",
      "Epoch 132/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 679.3765 - val_loss: 1131.7772\n",
      "Epoch 133/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 666.9792 - val_loss: 1500.1508\n",
      "Epoch 134/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 736.5900 - val_loss: 1135.0165\n",
      "Epoch 135/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 599.9988 - val_loss: 1191.3610\n",
      "Epoch 136/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 607.5010 - val_loss: 1134.8148\n",
      "Epoch 137/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 640.9180 - val_loss: 1312.4257\n",
      "Epoch 138/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 599.7137 - val_loss: 1181.6659\n",
      "Epoch 139/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 568.3337 - val_loss: 1153.3793\n",
      "Epoch 140/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 553.3834 - val_loss: 1173.3772\n",
      "Epoch 141/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 548.7921 - val_loss: 1150.2897\n",
      "Epoch 142/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 533.4434 - val_loss: 1139.0723\n",
      "Epoch 143/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 543.7051 - val_loss: 1157.8564\n",
      "Epoch 144/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 522.6405 - val_loss: 1159.5458\n",
      "Epoch 145/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 541.6998 - val_loss: 1127.7117\n",
      "Epoch 146/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 520.0267 - val_loss: 1180.4617\n",
      "Epoch 147/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 543.3574 - val_loss: 1138.2762\n",
      "Epoch 148/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 535.6532 - val_loss: 1132.3337\n",
      "Epoch 149/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 548.6225 - val_loss: 1131.1041\n",
      "Epoch 150/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 509.4703 - val_loss: 1100.8987\n",
      "Epoch 151/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 514.6759 - val_loss: 1179.5831\n",
      "Epoch 152/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 533.9319 - val_loss: 1187.4148\n",
      "Epoch 153/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 506.3230 - val_loss: 1128.3292\n",
      "Epoch 154/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 496.2519 - val_loss: 1141.7273\n",
      "Epoch 155/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 508.3146 - val_loss: 1265.8311\n",
      "Epoch 156/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 527.4026 - val_loss: 1182.5508\n",
      "Epoch 157/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 489.7950 - val_loss: 1137.9066\n",
      "Epoch 158/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 510.1549 - val_loss: 1152.1742\n",
      "Epoch 159/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 505.9203 - val_loss: 1160.8267\n",
      "Epoch 160/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 476.8168 - val_loss: 1156.0343\n",
      "Epoch 161/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 489.0445 - val_loss: 1151.1348\n",
      "Epoch 162/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 480.7400 - val_loss: 1178.4965\n",
      "Epoch 163/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 492.3499 - val_loss: 1216.9574\n",
      "Epoch 164/200\n",
      "135/135 [==============================] - 1s 4ms/step - loss: 515.9421 - val_loss: 1128.5323\n",
      "Epoch 165/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 527.0015 - val_loss: 1403.5120\n",
      "Epoch 166/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 682.7845 - val_loss: 1318.1890\n",
      "Epoch 167/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 743.0320 - val_loss: 1084.7291\n",
      "Epoch 168/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 552.4874 - val_loss: 1086.2759\n",
      "Epoch 169/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 529.8497 - val_loss: 1123.4569\n",
      "Epoch 170/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 479.9426 - val_loss: 1150.7053\n",
      "Epoch 171/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 471.5320 - val_loss: 1107.9430\n",
      "Epoch 172/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 474.2622 - val_loss: 1042.2603\n",
      "Epoch 173/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 476.8410 - val_loss: 1073.6102\n",
      "Epoch 174/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 456.8910 - val_loss: 1070.2893\n",
      "Epoch 175/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 443.1855 - val_loss: 1119.0065\n",
      "Epoch 176/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 451.8941 - val_loss: 1121.2017\n",
      "Epoch 177/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 427.0646 - val_loss: 1094.5853\n",
      "Epoch 178/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 441.6366 - val_loss: 1103.0507\n",
      "Epoch 179/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 469.8488 - val_loss: 1099.7523\n",
      "Epoch 180/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 429.6717 - val_loss: 1164.0624\n",
      "Epoch 181/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 448.5135 - val_loss: 1134.5934\n",
      "Epoch 182/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 445.2395 - val_loss: 1162.6534\n",
      "Epoch 183/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 453.5960 - val_loss: 1144.5248\n",
      "Epoch 184/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 428.0334 - val_loss: 1130.7548\n",
      "Epoch 185/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 437.7281 - val_loss: 1060.0959\n",
      "Epoch 186/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 405.5306 - val_loss: 1062.5045\n",
      "Epoch 187/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 445.1779 - val_loss: 1158.7029\n",
      "Epoch 188/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 403.2070 - val_loss: 1178.4545\n",
      "Epoch 189/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 421.5330 - val_loss: 1212.2408\n",
      "Epoch 190/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 435.6798 - val_loss: 1195.8209\n",
      "Epoch 191/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 438.8148 - val_loss: 1192.3739\n",
      "Epoch 192/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 421.8890 - val_loss: 1166.7202\n",
      "Epoch 193/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 408.1724 - val_loss: 1269.7421\n",
      "Epoch 194/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 454.0490 - val_loss: 1186.5565\n",
      "Epoch 195/200\n",
      "135/135 [==============================] - 0s 4ms/step - loss: 513.2815 - val_loss: 1184.0416\n",
      "Epoch 196/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 448.1682 - val_loss: 1167.8307\n",
      "Epoch 197/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 438.6773 - val_loss: 1201.0090\n",
      "Epoch 198/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 450.7528 - val_loss: 1342.3807\n",
      "Epoch 199/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 450.1723 - val_loss: 1365.3917\n",
      "Epoch 200/200\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 484.4120 - val_loss: 1602.1191\n",
      "34/34 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Using past 5 hours to predict the next hour's delay\n",
    "window_size = 8\n",
    "\n",
    "# Convert datetime to its components\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# One-hot encode the time variables\n",
    "data = pd.get_dummies(data, columns=['month', 'day', 'hour', 'dayofweek'])\n",
    "\n",
    "features = data[['rain', 'snowfall', 'windspeed_100m'] + [col for col in data.columns if 'month_' in col or 'day_' in col or 'hour_' in col or 'dayofweek_' in col]]\n",
    "X, y = create_sequences(features.values, window_size)\n",
    "y = data['DEP_DELAY'].values[window_size:]\n",
    "\n",
    "# Reshape X for LSTM [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], features.shape[1])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), shuffle=False)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 27.679236873767657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "147/147 [==============================] - 2s 6ms/step - loss: 1558.2350 - val_loss: 970.2261\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1536.9891 - val_loss: 964.0182\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1529.7279 - val_loss: 963.2305\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1522.5134 - val_loss: 963.9669\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1516.1556 - val_loss: 963.5460\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1511.2056 - val_loss: 961.0878\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1496.9464 - val_loss: 955.9028\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1486.3789 - val_loss: 976.0765\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1504.3022 - val_loss: 951.6445\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1478.6991 - val_loss: 940.8123\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1463.2515 - val_loss: 941.8488\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1454.4163 - val_loss: 933.5436\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1432.6539 - val_loss: 920.5767\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1424.1882 - val_loss: 925.7188\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1421.9683 - val_loss: 936.7271\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1419.2219 - val_loss: 918.9677\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1389.6373 - val_loss: 924.7092\n",
      "Epoch 18/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1377.4188 - val_loss: 925.8271\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1375.6500 - val_loss: 1001.0723\n",
      "Epoch 20/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1375.6953 - val_loss: 947.1293\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1358.7773 - val_loss: 936.1603\n",
      "Epoch 22/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1342.8804 - val_loss: 951.9315\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1345.7170 - val_loss: 940.0359\n",
      "Epoch 24/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1322.6300 - val_loss: 934.1393\n",
      "Epoch 25/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1283.4617 - val_loss: 927.5508\n",
      "Epoch 26/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1270.2400 - val_loss: 918.1680\n",
      "Epoch 27/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1301.8375 - val_loss: 893.6987\n",
      "Epoch 28/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1274.1823 - val_loss: 926.6931\n",
      "Epoch 29/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1245.3962 - val_loss: 923.2347\n",
      "Epoch 30/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1238.7350 - val_loss: 932.3406\n",
      "Epoch 31/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1293.7870 - val_loss: 920.9444\n",
      "Epoch 32/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1269.9686 - val_loss: 886.3110\n",
      "Epoch 33/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1234.2142 - val_loss: 952.7024\n",
      "Epoch 34/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1244.5397 - val_loss: 902.7968\n",
      "Epoch 35/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1235.5201 - val_loss: 930.3102\n",
      "Epoch 36/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1197.3784 - val_loss: 891.1364\n",
      "Epoch 37/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1228.6937 - val_loss: 848.1230\n",
      "Epoch 38/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1251.0449 - val_loss: 925.6920\n",
      "Epoch 39/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1263.5054 - val_loss: 904.1293\n",
      "Epoch 40/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1181.5732 - val_loss: 857.5001\n",
      "Epoch 41/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1132.0946 - val_loss: 895.2673\n",
      "Epoch 42/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1183.3798 - val_loss: 856.5461\n",
      "Epoch 43/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1122.1077 - val_loss: 836.4727\n",
      "Epoch 44/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1143.0692 - val_loss: 936.8738\n",
      "Epoch 45/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1159.7908 - val_loss: 855.6949\n",
      "Epoch 46/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1066.7605 - val_loss: 841.6245\n",
      "Epoch 47/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1125.3312 - val_loss: 1003.5996\n",
      "Epoch 48/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1191.3502 - val_loss: 957.9543\n",
      "Epoch 49/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1144.2982 - val_loss: 923.8608\n",
      "Epoch 50/50\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 1105.6340 - val_loss: 847.0934\n",
      "37/37 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Using past 5 hours to predict the next hour's delay\n",
    "window_size = 8\n",
    "\n",
    "# Convert datetime to its components\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "features = data[['month', 'day', 'hour', 'dayofweek', 'rain', 'snowfall', 'windspeed_100m']]\n",
    "X, y = create_sequences(features.values, window_size)\n",
    "y = data['DEP_DELAY'].values[window_size:]\n",
    "\n",
    "# Reshape X for LSTM [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], features.shape[1])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model with Stacked layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, strides=1, activation='relu', padding='causal', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(32, return_sequences=True)) # Add return_sequences=True for stacking\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False))  # Additional LSTM layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), shuffle=False)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 27.640011489633636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "147/147 [==============================] - 3s 6ms/step - loss: 0.0041 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0038 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "147/147 [==============================] - 1s 4ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "147/147 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 19/200\n",
      " 39/147 [======>.......................] - ETA: 0s - loss: 0.0051"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y135sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m reduce_lr \u001b[39m=\u001b[39m ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y135sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Train model with Callbacks\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y135sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_test, y_test), shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, callbacks\u001b[39m=\u001b[39m[early_stopping, reduce_lr])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y135sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Predict\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y135sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function\u001b[39m.\u001b[39mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Sample data loading (please ensure you have loaded your data into the 'data' variable)\n",
    "# data = pd.read_csv('your_data_path.csv')\n",
    "\n",
    "# Using past 8 hours to predict the next hour's delay\n",
    "window_size = 8\n",
    "\n",
    "# Convert datetime to its components\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "features = data[['month', 'day', 'hour', 'dayofweek', 'rain', 'snowfall', 'windspeed_100m']]\n",
    "X, y = create_sequences(features.values, window_size)\n",
    "y = data['DEP_DELAY'].values[window_size:]\n",
    "\n",
    "# Feature Standardization\n",
    "scaler_X = MinMaxScaler()\n",
    "X = scaler_X.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "scaler_y = MinMaxScaler()\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model with Stacked layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True)) # Add return_sequences=True for stacking\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(30, return_sequences=False))  # Additional LSTM layer\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train model with Callbacks\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), shuffle=False, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred)  # Inverse scaling for predictions\n",
    "y_test = scaler_y.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 42.26330723504036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Compute RMSE\n",
    "y_test = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "165/165 [==============================] - 2s 5ms/step - loss: 0.0040 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "165/165 [==============================] - 1s 4ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "113/165 [===================>..........] - ETA: 0s - loss: 0.0035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y124sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m reduce_lr \u001b[39m=\u001b[39m ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y124sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Train model with Callbacks\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y124sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_test, y_test), shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, callbacks\u001b[39m=\u001b[39m[early_stopping, reduce_lr])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y124sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Predict\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/saianoopavunuri/Documents/cs_projects/flight_delay_prediction/test2.ipynb#Y124sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function\u001b[39m.\u001b[39mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Sample data loading (please ensure you have loaded your data into the 'data' variable)\n",
    "# data = pd.read_csv('your_data_path.csv')\n",
    "\n",
    "# Using past 8 hours to predict the next hour's delay\n",
    "window_size = 8\n",
    "\n",
    "# Convert datetime to its components\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "features = data[['month', 'day', 'hour', 'dayofweek', 'rain', 'snowfall', 'windspeed_100m']]\n",
    "X, y = create_sequences(features.values, window_size)\n",
    "y = data['DEP_DELAY'].values[window_size:]\n",
    "\n",
    "# Feature Standardization\n",
    "scaler_X = MinMaxScaler()\n",
    "X = scaler_X.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "scaler_y = MinMaxScaler()\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build LSTM model with Stacked layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True)) # Add return_sequences=True for stacking\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False))  # Additional LSTM layer\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train model with Callbacks\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), shuffle=False, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred)  # Inverse scaling for predictions\n",
    "y_test = scaler_y.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 27.612384856338696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[  0   0   0   0]\n",
      " [229 819  23   6]\n",
      " [ 10  30   4   1]\n",
      " [  3  27   5  14]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.93      0.76      0.84      1077\n",
      "           2       0.12      0.09      0.10        45\n",
      "           3       0.67      0.29      0.40        49\n",
      "\n",
      "    accuracy                           0.71      1171\n",
      "   macro avg       0.43      0.28      0.34      1171\n",
      "weighted avg       0.89      0.71      0.79      1171\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.7147736976942783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saianoopavunuri/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saianoopavunuri/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saianoopavunuri/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Sample data\n",
    "y_true = y_test\n",
    "\n",
    "# Define a function to bin the values\n",
    "def bin_values(values):\n",
    "    bins = [0, 30, 60, float('inf')]\n",
    "    labels = ['0-30', '30-60', '60+']\n",
    "    return np.digitize(values, bins=bins, right=False).astype(str)\n",
    "\n",
    "# Bin the true and predicted values\n",
    "binned_y_true = bin_values(y_true)\n",
    "binned_y_pred = bin_values(y_pred)\n",
    "\n",
    "# Calculate classification metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(binned_y_true, binned_y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(binned_y_true, binned_y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(binned_y_true, binned_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = requests.get('https://archive-api.open-meteo.com/v1/archive?latitude=33.75&longitude=-84.39&start_date=2017-01-01&end_date=2018-12-31&hourly=temperature_2m,rain,snowfall,cloudcover,windspeed_100m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather2 = response2.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {}\n",
    "from datetime import datetime\n",
    "for i, date in enumerate(weather2['hourly']['time']):\n",
    "    hour  = datetime.strptime(date, \"%Y-%m-%dT%H:%M\")\n",
    "    weather_dict[hour] = {'temperature_2m': weather2['hourly']['temperature_2m'][i],'rain': weather2['hourly']['rain'][i], 'snowfall': weather2['hourly']['snowfall'][i], 'cloudcover': weather2['hourly']['cloudcover'][i], 'windspeed_100m': weather2['hourly']['windspeed_100m'][i] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floor the datetime to the nearest hour\n",
    "data[\"floored_datetime\"] = data[\"datetime\"].dt.floor(\"H\") + pd.Timedelta(hours=2)\n",
    "\n",
    "# Look up the weather data and add new columns to the DataFrame\n",
    "for feature in ['temperature_2m', 'rain', 'snowfall', 'cloudcover', 'windspeed_100m','precipitation']:\n",
    "    data[feature+\"_arr\" + \"_2hr\"] = data[\"floored_datetime\"].map(lambda x: weather_dict.get(x, {}).get(feature, None))\n",
    "\n",
    "# Drop the 'floored_datetime' column if not needed\n",
    "data.drop(columns=[\"floored_datetime\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'FL_DATE', 'OP_CARRIER', 'OP_CARRIER_FL_NUM', 'ORIGIN', 'DEST',\n",
       "       'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY',\n",
       "       'CRS_ELAPSED_TIME', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE',\n",
       "       'tiCRS_DEP_TIMEme', 'datetime', 'temperature_2m', 'rain', 'snowfall',\n",
       "       'cloudcover', 'windspeed_100m', 'precipitation',\n",
       "       'temperature_2m_arr_2hr', 'rain_arr_2hr', 'snowfall_arr_2hr',\n",
       "       'cloudcover_arr_2hr', 'windspeed_100m_arr_2hr', 'precipitation_arr_2hr',\n",
       "       'year', 'month', 'day', 'hour', 'dayofweek'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "146/146 [==============================] - 2s 4ms/step - loss: 1442.9301 - val_loss: 1279.8098\n",
      "Epoch 2/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1417.0137 - val_loss: 1270.4115\n",
      "Epoch 3/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 1412.0981 - val_loss: 1263.8053\n",
      "Epoch 4/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1407.2360 - val_loss: 1258.7118\n",
      "Epoch 5/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 1402.2797 - val_loss: 1248.0328\n",
      "Epoch 6/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1394.9066 - val_loss: 1249.3698\n",
      "Epoch 7/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1394.8107 - val_loss: 1240.4829\n",
      "Epoch 8/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1382.7849 - val_loss: 1234.9933\n",
      "Epoch 9/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1381.3826 - val_loss: 1229.1936\n",
      "Epoch 10/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1374.5271 - val_loss: 1223.6265\n",
      "Epoch 11/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1372.6145 - val_loss: 1219.6465\n",
      "Epoch 12/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1366.4662 - val_loss: 1217.8152\n",
      "Epoch 13/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1368.4520 - val_loss: 1207.0319\n",
      "Epoch 14/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1356.7634 - val_loss: 1203.6299\n",
      "Epoch 15/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1354.2601 - val_loss: 1203.4180\n",
      "Epoch 16/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1358.9354 - val_loss: 1204.0502\n",
      "Epoch 17/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1345.7529 - val_loss: 1190.1091\n",
      "Epoch 18/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1344.5736 - val_loss: 1186.2638\n",
      "Epoch 19/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1335.1376 - val_loss: 1180.4773\n",
      "Epoch 20/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1342.0242 - val_loss: 1180.6586\n",
      "Epoch 21/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1322.7776 - val_loss: 1167.5521\n",
      "Epoch 22/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1310.9320 - val_loss: 1162.7095\n",
      "Epoch 23/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1305.9275 - val_loss: 1164.3430\n",
      "Epoch 24/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1302.6740 - val_loss: 1154.4987\n",
      "Epoch 25/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1296.2559 - val_loss: 1156.4930\n",
      "Epoch 26/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1291.0413 - val_loss: 1140.1882\n",
      "Epoch 27/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1295.8549 - val_loss: 1131.6185\n",
      "Epoch 28/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1278.0576 - val_loss: 1129.1722\n",
      "Epoch 29/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1264.1118 - val_loss: 1125.6866\n",
      "Epoch 30/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1269.3668 - val_loss: 1125.1233\n",
      "Epoch 31/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1251.8678 - val_loss: 1109.5388\n",
      "Epoch 32/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1249.1857 - val_loss: 1122.9995\n",
      "Epoch 33/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1250.7931 - val_loss: 1117.5391\n",
      "Epoch 34/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1240.5856 - val_loss: 1102.2030\n",
      "Epoch 35/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1227.5073 - val_loss: 1092.6195\n",
      "Epoch 36/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 1211.7421 - val_loss: 1081.6788\n",
      "Epoch 37/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1213.3279 - val_loss: 1099.7612\n",
      "Epoch 38/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1214.0367 - val_loss: 1074.4128\n",
      "Epoch 39/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1196.2588 - val_loss: 1079.2930\n",
      "Epoch 40/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1183.3512 - val_loss: 1063.5190\n",
      "Epoch 41/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1185.3513 - val_loss: 1076.6235\n",
      "Epoch 42/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1173.8046 - val_loss: 1070.1505\n",
      "Epoch 43/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1178.2028 - val_loss: 1075.6965\n",
      "Epoch 44/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1206.7523 - val_loss: 1136.7141\n",
      "Epoch 45/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1202.4105 - val_loss: 1099.8907\n",
      "Epoch 46/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1182.7689 - val_loss: 1052.1273\n",
      "Epoch 47/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1167.2274 - val_loss: 1054.6693\n",
      "Epoch 48/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1158.9443 - val_loss: 1055.0627\n",
      "Epoch 49/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1141.8516 - val_loss: 1052.1942\n",
      "Epoch 50/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1161.8813 - val_loss: 1105.0148\n",
      "Epoch 51/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1149.6620 - val_loss: 1039.2979\n",
      "Epoch 52/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1123.1248 - val_loss: 1038.2294\n",
      "Epoch 53/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1130.6924 - val_loss: 1030.3447\n",
      "Epoch 54/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1104.0293 - val_loss: 1032.3834\n",
      "Epoch 55/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1105.9580 - val_loss: 1031.3234\n",
      "Epoch 56/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1112.4579 - val_loss: 1081.1951\n",
      "Epoch 57/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 1107.1895 - val_loss: 1057.0688\n",
      "Epoch 58/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1151.7938 - val_loss: 1105.2814\n",
      "Epoch 59/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1146.7147 - val_loss: 1042.0244\n",
      "Epoch 60/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1089.1422 - val_loss: 1027.5002\n",
      "Epoch 61/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1081.0306 - val_loss: 1002.8943\n",
      "Epoch 62/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1074.1827 - val_loss: 1014.6941\n",
      "Epoch 63/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1079.0399 - val_loss: 1006.6234\n",
      "Epoch 64/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1066.3610 - val_loss: 1025.0751\n",
      "Epoch 65/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1047.6149 - val_loss: 1005.9243\n",
      "Epoch 66/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1044.9752 - val_loss: 1011.2817\n",
      "Epoch 67/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 1108.9186 - val_loss: 1012.0190\n",
      "Epoch 68/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1054.9945 - val_loss: 983.9683\n",
      "Epoch 69/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1044.6096 - val_loss: 982.3565\n",
      "Epoch 70/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1018.7850 - val_loss: 1008.5784\n",
      "Epoch 71/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1015.6580 - val_loss: 1069.3105\n",
      "Epoch 72/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1076.7623 - val_loss: 1005.7558\n",
      "Epoch 73/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1133.6056 - val_loss: 1042.1146\n",
      "Epoch 74/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1039.0709 - val_loss: 1006.8383\n",
      "Epoch 75/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1016.1967 - val_loss: 993.3903\n",
      "Epoch 76/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 993.9390 - val_loss: 988.5643\n",
      "Epoch 77/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1011.6300 - val_loss: 997.5394\n",
      "Epoch 78/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 985.4986 - val_loss: 978.8414\n",
      "Epoch 79/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 988.4640 - val_loss: 1024.1223\n",
      "Epoch 80/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1117.2117 - val_loss: 1133.0549\n",
      "Epoch 81/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1085.6620 - val_loss: 1029.3757\n",
      "Epoch 82/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 992.1684 - val_loss: 1022.0601\n",
      "Epoch 83/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1009.1483 - val_loss: 1025.4292\n",
      "Epoch 84/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 971.6105 - val_loss: 986.1607\n",
      "Epoch 85/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 998.9897 - val_loss: 1034.9200\n",
      "Epoch 86/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 956.8561 - val_loss: 1011.3110\n",
      "Epoch 87/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 933.4540 - val_loss: 1010.9659\n",
      "Epoch 88/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 919.2499 - val_loss: 1016.2873\n",
      "Epoch 89/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 942.6876 - val_loss: 969.0649\n",
      "Epoch 90/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 915.4465 - val_loss: 970.0075\n",
      "Epoch 91/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 918.4597 - val_loss: 998.5330\n",
      "Epoch 92/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 938.2330 - val_loss: 971.4363\n",
      "Epoch 93/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1003.7803 - val_loss: 1000.1448\n",
      "Epoch 94/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 957.6624 - val_loss: 996.3730\n",
      "Epoch 95/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 917.0267 - val_loss: 990.9100\n",
      "Epoch 96/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 898.5026 - val_loss: 979.3790\n",
      "Epoch 97/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 908.8985 - val_loss: 1103.3080\n",
      "Epoch 98/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1008.9026 - val_loss: 1027.3584\n",
      "Epoch 99/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 956.5764 - val_loss: 1090.2560\n",
      "Epoch 100/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 935.3166 - val_loss: 1004.2277\n",
      "Epoch 101/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 898.2991 - val_loss: 966.5901\n",
      "Epoch 102/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 891.0193 - val_loss: 1038.7981\n",
      "Epoch 103/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 868.7980 - val_loss: 964.4034\n",
      "Epoch 104/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 879.3279 - val_loss: 980.0305\n",
      "Epoch 105/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 893.8697 - val_loss: 1175.5771\n",
      "Epoch 106/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 912.2813 - val_loss: 997.8016\n",
      "Epoch 107/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 959.5891 - val_loss: 1044.7046\n",
      "Epoch 108/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 900.4310 - val_loss: 947.5055\n",
      "Epoch 109/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 832.9707 - val_loss: 1038.9579\n",
      "Epoch 110/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 869.9642 - val_loss: 994.0744\n",
      "Epoch 111/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 853.9209 - val_loss: 1010.2144\n",
      "Epoch 112/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 865.1722 - val_loss: 978.3408\n",
      "Epoch 113/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 866.5591 - val_loss: 1017.2914\n",
      "Epoch 114/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 834.9969 - val_loss: 1121.7822\n",
      "Epoch 115/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 893.4568 - val_loss: 972.5065\n",
      "Epoch 116/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 841.4028 - val_loss: 1000.4918\n",
      "Epoch 117/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1040.0347 - val_loss: 1094.3489\n",
      "Epoch 118/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 930.4280 - val_loss: 1015.7219\n",
      "Epoch 119/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 872.9537 - val_loss: 1032.6395\n",
      "Epoch 120/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 960.6542 - val_loss: 1008.7714\n",
      "Epoch 121/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 861.0366 - val_loss: 1025.6498\n",
      "Epoch 122/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 848.3237 - val_loss: 1005.1378\n",
      "Epoch 123/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 847.1888 - val_loss: 1048.9630\n",
      "Epoch 124/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 815.6484 - val_loss: 1077.5227\n",
      "Epoch 125/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 848.2275 - val_loss: 1040.3674\n",
      "Epoch 126/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1131.1108 - val_loss: 1249.6119\n",
      "Epoch 127/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1246.7450 - val_loss: 1197.4563\n",
      "Epoch 128/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1138.5433 - val_loss: 1021.7127\n",
      "Epoch 129/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 966.7672 - val_loss: 1088.5211\n",
      "Epoch 130/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 900.6757 - val_loss: 1084.0782\n",
      "Epoch 131/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 928.7562 - val_loss: 1166.0062\n",
      "Epoch 132/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 911.2352 - val_loss: 1118.4708\n",
      "Epoch 133/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 972.6046 - val_loss: 997.6816\n",
      "Epoch 134/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 838.3870 - val_loss: 1022.3849\n",
      "Epoch 135/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 813.9446 - val_loss: 990.8989\n",
      "Epoch 136/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 832.2687 - val_loss: 1075.5482\n",
      "Epoch 137/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 794.3447 - val_loss: 1070.5548\n",
      "Epoch 138/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 825.5128 - val_loss: 968.7057\n",
      "Epoch 139/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 819.6093 - val_loss: 959.1356\n",
      "Epoch 140/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 841.4748 - val_loss: 1064.2098\n",
      "Epoch 141/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 779.4882 - val_loss: 1021.4647\n",
      "Epoch 142/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 780.0075 - val_loss: 1044.9565\n",
      "Epoch 143/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 758.8403 - val_loss: 1022.8611\n",
      "Epoch 144/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 746.6625 - val_loss: 1047.8340\n",
      "Epoch 145/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 754.2953 - val_loss: 1139.5408\n",
      "Epoch 146/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 946.1483 - val_loss: 1008.3909\n",
      "Epoch 147/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 797.1813 - val_loss: 1125.8969\n",
      "Epoch 148/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 740.5524 - val_loss: 1039.1366\n",
      "Epoch 149/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 732.9645 - val_loss: 1023.2178\n",
      "Epoch 150/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 762.2405 - val_loss: 974.1810\n",
      "Epoch 151/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 781.4158 - val_loss: 1045.5975\n",
      "Epoch 152/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 950.0079 - val_loss: 1162.7468\n",
      "Epoch 153/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 857.4962 - val_loss: 1136.2605\n",
      "Epoch 154/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 862.3960 - val_loss: 1004.2332\n",
      "Epoch 155/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 999.3265 - val_loss: 1035.6305\n",
      "Epoch 156/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 939.3174 - val_loss: 1012.2979\n",
      "Epoch 157/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 837.5401 - val_loss: 1011.4670\n",
      "Epoch 158/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 755.3222 - val_loss: 1026.6309\n",
      "Epoch 159/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 845.7263 - val_loss: 1149.1250\n",
      "Epoch 160/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 794.0634 - val_loss: 934.2263\n",
      "Epoch 161/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 761.8606 - val_loss: 967.7093\n",
      "Epoch 162/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 725.8085 - val_loss: 991.6102\n",
      "Epoch 163/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 704.4971 - val_loss: 979.4016\n",
      "Epoch 164/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 710.7554 - val_loss: 1075.9956\n",
      "Epoch 165/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 722.2349 - val_loss: 990.2764\n",
      "Epoch 166/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 707.4876 - val_loss: 993.3297\n",
      "Epoch 167/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 706.1407 - val_loss: 965.7773\n",
      "Epoch 168/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 744.1148 - val_loss: 1151.6534\n",
      "Epoch 169/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 729.4871 - val_loss: 1030.8060\n",
      "Epoch 170/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 703.8162 - val_loss: 939.9838\n",
      "Epoch 171/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 681.7797 - val_loss: 1057.2245\n",
      "Epoch 172/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 748.6577 - val_loss: 1015.2647\n",
      "Epoch 173/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 882.1330 - val_loss: 1134.4222\n",
      "Epoch 174/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 884.8306 - val_loss: 1034.5468\n",
      "Epoch 175/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 866.7421 - val_loss: 972.3517\n",
      "Epoch 176/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 815.8298 - val_loss: 919.8226\n",
      "Epoch 177/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 686.6661 - val_loss: 970.6024\n",
      "Epoch 178/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 671.3446 - val_loss: 973.2730\n",
      "Epoch 179/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 674.0193 - val_loss: 972.7703\n",
      "Epoch 180/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 780.9059 - val_loss: 1275.6399\n",
      "Epoch 181/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1134.1672 - val_loss: 1297.9573\n",
      "Epoch 182/200\n",
      "146/146 [==============================] - 1s 4ms/step - loss: 1118.7063 - val_loss: 1006.2196\n",
      "Epoch 183/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 1038.9342 - val_loss: 1201.5886\n",
      "Epoch 184/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 847.3568 - val_loss: 965.7876\n",
      "Epoch 185/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 722.1420 - val_loss: 983.2029\n",
      "Epoch 186/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 703.1450 - val_loss: 997.7688\n",
      "Epoch 187/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 861.4445 - val_loss: 990.1752\n",
      "Epoch 188/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 677.0287 - val_loss: 983.1097\n",
      "Epoch 189/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 685.8637 - val_loss: 1019.3164\n",
      "Epoch 190/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 651.3878 - val_loss: 1114.1624\n",
      "Epoch 191/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 652.1338 - val_loss: 941.6990\n",
      "Epoch 192/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 637.3613 - val_loss: 936.1498\n",
      "Epoch 193/200\n",
      "146/146 [==============================] - 0s 3ms/step - loss: 656.9388 - val_loss: 1070.4004\n",
      "Epoch 194/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 681.9725 - val_loss: 994.0038\n",
      "Epoch 195/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 623.4703 - val_loss: 968.2695\n",
      "Epoch 196/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 629.3242 - val_loss: 1027.8729\n",
      "Epoch 197/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 613.3545 - val_loss: 1042.4888\n",
      "Epoch 198/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 627.0276 - val_loss: 1199.6567\n",
      "Epoch 199/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 790.0782 - val_loss: 1262.8058\n",
      "Epoch 200/200\n",
      "146/146 [==============================] - 0s 2ms/step - loss: 716.5130 - val_loss: 1145.6287\n",
      "37/37 [==============================] - 0s 983us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Using past 5 hours to predict the next hour's delay\n",
    "window_size = 8\n",
    "\n",
    "# Convert datetime to its components\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['day'] = data['datetime'].dt.day\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['dayofweek'] = data['datetime'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "features = data[['month', 'day', 'hour', 'dayofweek', 'rain', 'snowfall', 'windspeed_100m', 'rain_arr_2hr','snowfall_arr_2hr', 'windspeed_100m_arr_2hr']]\n",
    "X, y = create_sequences(features.values, window_size)\n",
    "y = data['DEP_DELAY'].values[window_size:]\n",
    "\n",
    "# Reshape X for LSTM [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], features.shape[1])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), shuffle=False)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 33.847136609376165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>...</th>\n",
       "      <th>rain_arr_2hr</th>\n",
       "      <th>snowfall_arr_2hr</th>\n",
       "      <th>cloudcover_arr_2hr</th>\n",
       "      <th>windspeed_100m_arr_2hr</th>\n",
       "      <th>precipitation_arr_2hr</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8463</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1827</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>06:30</td>\n",
       "      <td>631.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>29.5</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7839</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1109</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>08:00</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>12.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8646</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>2032</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>08:30</td>\n",
       "      <td>827.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>12.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8460</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1824</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>09:20</td>\n",
       "      <td>919.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>13.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8462</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>DL</td>\n",
       "      <td>1826</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>10:25</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>15.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>5662786</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>82</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>12:20</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>12.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5911</th>\n",
       "      <td>5663163</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>1058</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>14:20</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>8.9</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>5662782</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>72</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>15:20</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>5664379</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>2586</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>16:25</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>16.9</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5914</th>\n",
       "      <td>5664078</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>DL</td>\n",
       "      <td>2172</td>\n",
       "      <td>MCO</td>\n",
       "      <td>ATL</td>\n",
       "      <td>17:25</td>\n",
       "      <td>1723.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>15.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5821 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  \\\n",
       "6        8463  2017-01-01         DL               1827    MCO  ATL   \n",
       "3        7839  2017-01-01         DL               1109    MCO  ATL   \n",
       "13       8646  2017-01-01         DL               2032    MCO  ATL   \n",
       "4        8460  2017-01-01         DL               1824    MCO  ATL   \n",
       "5        8462  2017-01-01         DL               1826    MCO  ATL   \n",
       "...       ...         ...        ...                ...    ...  ...   \n",
       "5910  5662786  2017-12-31         DL                 82    MCO  ATL   \n",
       "5911  5663163  2017-12-31         DL               1058    MCO  ATL   \n",
       "5909  5662782  2017-12-31         DL                 72    MCO  ATL   \n",
       "5917  5664379  2017-12-31         DL               2586    MCO  ATL   \n",
       "5914  5664078  2017-12-31         DL               2172    MCO  ATL   \n",
       "\n",
       "     CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  ...  rain_arr_2hr  \\\n",
       "6           06:30     631.0        1.0       8.0  ...           1.2   \n",
       "3           08:00     800.0        0.0      14.0  ...           0.4   \n",
       "13          08:30     827.0        0.0      11.0  ...           0.4   \n",
       "4           09:20     919.0        0.0      19.0  ...           0.2   \n",
       "5           10:25    1023.0        0.0      12.0  ...           0.0   \n",
       "...           ...       ...        ...       ...  ...           ...   \n",
       "5910        12:20    1212.0        0.0      12.0  ...           0.0   \n",
       "5911        14:20    1419.0        0.0      18.0  ...           0.0   \n",
       "5909        15:20    1517.0        0.0      26.0  ...           0.0   \n",
       "5917        16:25    1623.0        0.0      14.0  ...           0.1   \n",
       "5914        17:25    1723.0        0.0      11.0  ...           0.0   \n",
       "\n",
       "      snowfall_arr_2hr  cloudcover_arr_2hr  windspeed_100m_arr_2hr  \\\n",
       "6                  0.0                 100                    29.5   \n",
       "3                  0.0                 100                    12.6   \n",
       "13                 0.0                 100                    12.6   \n",
       "4                  0.0                 100                    13.6   \n",
       "5                  0.0                 100                    15.6   \n",
       "...                ...                 ...                     ...   \n",
       "5910               0.0                 100                    12.6   \n",
       "5911               0.0                 100                     8.9   \n",
       "5909               0.0                 100                    10.0   \n",
       "5917               0.0                 100                    16.9   \n",
       "5914               0.0                 100                    15.1   \n",
       "\n",
       "      precipitation_arr_2hr  year  month  day  hour  dayofweek  \n",
       "6                      None  2017      1    1     6          6  \n",
       "3                      None  2017      1    1     8          6  \n",
       "13                     None  2017      1    1     8          6  \n",
       "4                      None  2017      1    1     9          6  \n",
       "5                      None  2017      1    1    10          6  \n",
       "...                     ...   ...    ...  ...   ...        ...  \n",
       "5910                   None  2017     12   31    12          6  \n",
       "5911                   None  2017     12   31    14          6  \n",
       "5909                   None  2017     12   31    15          6  \n",
       "5917                   None  2017     12   31    16          6  \n",
       "5914                   None  2017     12   31    17          6  \n",
       "\n",
       "[5821 rows x 39 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
